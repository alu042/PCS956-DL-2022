{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb3be86",
   "metadata": {},
   "source": [
    "A.S. Lundervold, v.011122"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2b9ea",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05754006",
   "metadata": {},
   "source": [
    "This is a quick example of some techniques and ideas from natural language processing (NLP) and some approaches to NLP based on deep learning. The goal is to introduce some of the things going on in this field and for you to better understand some recent ideas and developments in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46c391",
   "metadata": {},
   "source": [
    "> NLP is an exciting area these days. Breakthroughs in deep learning for language processing recently initiated a revolution in NLP, and we're still in it. The best place to start exploring this is perhaps the HuggingFace community and library (at least if you want to get started right away playing around with using state-of-the-art NLP models): https://huggingface.co/. <br> <a href=\"https://huggingface.co/\"><img width=20% src=\"https://luxcapital-website-media.s3.amazonaws.com/wp-content/uploads/2019/12/23115642/Logo-600x554.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3bd56",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39baafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ba40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17260248",
   "metadata": {},
   "source": [
    "We'll use the excellent HuggingFace Transformers library, which covers all our natural language processing needs:\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/b253a30b83a0724f3f74f3f58236fb49ced8d7b27cb15835c9978b54e444ab08/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e67666163652f646f63756d656e746174696f6e2d696d616765732f7265736f6c76652f6d61696e2f7472616e73666f726d6572735f6c6f676f5f6e616d652e706e67\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad307da5",
   "metadata": {},
   "source": [
    "We will not cover the library in any detail. If you're interested, take a look at the [HuggingFace course](https://huggingface.co/course/chapter1/1) and its excellent documentation over at https://huggingface.co/transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c0448",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fcf954",
   "metadata": {},
   "source": [
    "We'll use the [IMDB dataset](https://huggingface.co/datasets/imdb) containing 50.000 movie reviews from IMDB, each labeled as either negative (0) or positive (1). It is split into 25.000 reviews for training and 25.000 reviews for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f23cb",
   "metadata": {},
   "source": [
    "The dataset is available via HuggingFace `datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f8a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3df4a28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb5355d85d34dac879e56e633f93d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90aae66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c3719",
   "metadata": {},
   "source": [
    "## Make a sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99509d",
   "metadata": {},
   "source": [
    "As the training process takes a long time, we create a small sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e2e3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd817458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-c676b5ef844040e4.arrow and /home/ubuntu/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-2711e3ecf04e8063.arrow\n"
     ]
    }
   ],
   "source": [
    "if sample:\n",
    "    dataset = dataset['train']\n",
    "    dataset = dataset.train_test_split(train_size=0.2, shuffle=True, seed=42)['train']\n",
    "    dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a714e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e357cc5",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4942af",
   "metadata": {},
   "source": [
    "The training data is stored under `train`, the test data under `test`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c430f0",
   "metadata": {},
   "source": [
    "Here are two training instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753a84a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Like some of the other reviewers have alluded to previously, I\\'d like to know what moron actually read the script and went\\', \"Yea!!! This is it. This is the next film we are going to green light!!\" And whoever that person is, should have his or her head examined for actual brain activity. Because whoever is responsible for actually dishing out money to have this made after reading the script, well, I\\'d love to give you my email address and maybe you\\'d like to just give away some more money. This film is atrocious in every way.<br /><br />The Wayans are funny, at least they can be. They have made some good films and had some incredibly funny performances along the way. But in here, not only does the premise defy all logic, not only is the acting terrible, not only is the entire movie offensive from start to finish, not only is the direction as amateurish as you can find, but they actually want you to pay to see this film. Maybe if it was free...naaah, it would still be a waste of time.<br /><br />Usually I\\'d be inclined to write some long winded, detailed review about why this film is so bad, but just suffice to say that let my brevity do the talking. This is the lowest common denominator film making and it is about as unfunny as a heart attack.<br /><br />0/10..makes my top ten list of worst films of all time!',\n",
       "  \"Sondra Locke stinks in this film, but then she was an awful 'actress' anyway. Unfortunately, she drags everyone else (including then =real life boyfriend Clint Eastwood down the drain with her. But what was Clint Eastwood thinking when he agreed to star in this one? One read of the script should have told him that this one was going to be a real snorer. It's an exceptionally weak story, basically no story or plot at all. Add in bored, poor acting, even from the normally good Eastwood. There's absolutely no action except a couple arguments and as far as I was concerned, this film ranks up at the top of the heap of natural sleep enhancers. Wow! Could a film BE any more boring? I think watching paint dry or the grass grow might be more fun. A real stinker. Don't bother with this one.\"],\n",
       " 'label': [0, 0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86615d",
   "metadata": {},
   "source": [
    "We can print them a in a more readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d137e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0ee9562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': \"Like some of the other reviewers have alluded to previously, I'd \"\n",
      "         'like to know what moron actually read the script and went\\', \"Yea!!! '\n",
      "         'This is it. This is the next film we are going to green light!!\" And '\n",
      "         'whoever that person is, should have his or her head examined for '\n",
      "         'actual brain activity. Because whoever is responsible for actually '\n",
      "         'dishing out money to have this made after reading the script, well, '\n",
      "         \"I'd love to give you my email address and maybe you'd like to just \"\n",
      "         'give away some more money. This film is atrocious in every way.<br '\n",
      "         '/><br />The Wayans are funny, at least they can be. They have made '\n",
      "         'some good films and had some incredibly funny performances along the '\n",
      "         'way. But in here, not only does the premise defy all logic, not only '\n",
      "         'is the acting terrible, not only is the entire movie offensive from '\n",
      "         'start to finish, not only is the direction as amateurish as you can '\n",
      "         'find, but they actually want you to pay to see this film. Maybe if '\n",
      "         'it was free...naaah, it would still be a waste of time.<br /><br '\n",
      "         \"/>Usually I'd be inclined to write some long winded, detailed review \"\n",
      "         'about why this film is so bad, but just suffice to say that let my '\n",
      "         'brevity do the talking. This is the lowest common denominator film '\n",
      "         'making and it is about as unfunny as a heart attack.<br /><br '\n",
      "         '/>0/10..makes my top ten list of worst films of all time!'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset['train'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a25a69",
   "metadata": {},
   "source": [
    "> **How do we represent the text for consumption by a machine learning model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d1df1",
   "metadata": {},
   "source": [
    "> **How can a computer read??**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4793ff9",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/7d5ed540c87d660cae46ca0d2055d760f786bea36513bb1a0b0784d47cef45b1/687474703a2f2f322e62702e626c6f6773706f742e636f6d2f5f2d2d75564865746b5549512f54446165356a476e6138492f4141414141414141414b302f734253704c7564576d63772f73313630302f72656164696e672e676966\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3de3a",
   "metadata": {},
   "source": [
    "# Prepare the data: tokenization and numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24507e5f",
   "metadata": {},
   "source": [
    "For a computer, everything is numbers. We have to convert the text to a series of numbers and then feed those to the computer.\n",
    "\n",
    "This can be done in two widely used steps in natural language processing: **tokenization** and **numericalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106b8e4",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53d947",
   "metadata": {},
   "source": [
    "In tokenization, the text is split into single words called tokens. A simple way to achieve this is to separate according to spaces in the text. But then we, among other things, lose punctuation and the fact that some words are contractions of multiple words (for example \"isn't\" and \"don't\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea543c",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/6c79dd15098f840a49149649832fa0efd7252d71d03257b5fc96379f7456d889/68747470733a2f2f73706163792e696f2f746f6b656e697a6174696f6e2d35376536313862643739643933336334636364333038623537333930363264362e737667\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4844a10",
   "metadata": {},
   "source": [
    "Multiple tokenization strategies can tackle these and other issues, for example, **rule-based splitting of sentences** (used by ULMFiT and Transformer XL and others), **Byte-Pair encoding** (used by GPT-2 and others), **WordPiece** (used by BERT and others), and **SentencePiece** (used by XLM and others)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a3f53",
   "metadata": {},
   "source": [
    "### Rule-based splitting of sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afba63c",
   "metadata": {},
   "source": [
    "The NLP library `spaCy` can help us with this kind of tokenization. We install spaCy and download a set of rules for tokenizing English text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a3e3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "%pip install spacy\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be7010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4180dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0d5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Here's a sentence to be tokenized by a tokenizer, and it includes the non-existent word graffalacticus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e01f1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "'s\n",
      "a\n",
      "sentence\n",
      "to\n",
      "be\n",
      "tokenized\n",
      "by\n",
      "a\n",
      "tokenizer\n",
      ",\n",
      "and\n",
      "it\n",
      "includes\n",
      "the\n",
      "non\n",
      "-\n",
      "existent\n",
      "word\n",
      "graffalacticus\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(example_sentence)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499d0c6",
   "metadata": {},
   "source": [
    "### Subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4b0c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bac4279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c704cc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'to',\n",
       " 'be',\n",
       " 'token',\n",
       " '##ized',\n",
       " 'by',\n",
       " 'a',\n",
       " 'token',\n",
       " '##izer',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'includes',\n",
       " 'the',\n",
       " 'non',\n",
       " '-',\n",
       " 'existent',\n",
       " 'word',\n",
       " 'graf',\n",
       " '##fa',\n",
       " '##la',\n",
       " '##ctic',\n",
       " '##us']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a4bb4",
   "metadata": {},
   "source": [
    "### Byte-Pair encoding: an example of training an encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab9231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65d3feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96915623",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5efa4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d494501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(dataset['train']['text'],trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e625fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence_bpe = tokenizer.encode(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "659bb141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=27, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42cc05a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'to',\n",
       " 'be',\n",
       " 'token',\n",
       " 'ized',\n",
       " 'by',\n",
       " 'a',\n",
       " 'token',\n",
       " 'izer',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'includes',\n",
       " 'the',\n",
       " 'non',\n",
       " '-',\n",
       " 'existent',\n",
       " 'word',\n",
       " 'gra',\n",
       " 'ff',\n",
       " 'al',\n",
       " 'actic',\n",
       " 'us']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_bpe.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f90155e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2011, 8, 84, 66, 11053, 157, 172, 15599, 1418, 269, 66, 15599, 15855, 13, 155]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_bpe.ids[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a68b6f",
   "metadata": {},
   "source": [
    "## Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e7464",
   "metadata": {},
   "source": [
    "We convert tokens to numbers by making a list of all the tokens that have been used and assign them to numbers. This has already been taken care of for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3016622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2011, 8, 84, 66, 11053, 157, 172, 15599, 1418, 269, 66, 15599, 15855, 13, 155]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_bpe.ids[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2029eb",
   "metadata": {},
   "source": [
    "# Embeddings and using pre-trained text encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b51e19",
   "metadata": {},
   "source": [
    "http://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5d1b9",
   "metadata": {},
   "source": [
    "# Fine-tuning pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511a44a",
   "metadata": {},
   "source": [
    "The advent of the **Transformers models** (see [here]() for a quick intro) has revolutionized the field of natural language processing. Therefore, when faced with any NLP task for which deep learning is applicable, everyone tends to turn to Transformers models. Furthermore, one typically uses _pre-trained models_. In other words, models that have already been trained on large-scale NLP tasks and thus contain representations that typically provide useful starting points for new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559300e5",
   "metadata": {},
   "source": [
    "## Text representation for pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4b8e9",
   "metadata": {},
   "source": [
    "When using pre-trained models, we must pre-process the text exactly as expected by the model. In other words, that we use the expected tokenization, numericalization, padding, and truncation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2858fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6c5d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbee4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "212463ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31012d3bda0147d4ac03ad2ea4b19712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ed7c78e0624232b883e9d0d2c9e245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eac1be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab40400",
   "metadata": {},
   "source": [
    "## Fine-tune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7859f0",
   "metadata": {},
   "source": [
    "We'll fine-tune a BERT model on our IMDB dataset. (Note that this is where it's best to use a sample of the dataset. Otherwise the training process will take a long time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2c882c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778b6d8",
   "metadata": {},
   "source": [
    "**Define the model and its preprocessing steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eae28f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a21b6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56633d01",
   "metadata": {},
   "source": [
    "**Set up our evaluation metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47a58029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b2e7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9302d",
   "metadata": {},
   "source": [
    "**Configure the training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "851fe61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c397261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e32f2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\".\", num_train_epochs=1, evaluation_strategy=\"epoch\", report_to='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d77a7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75ac8f",
   "metadata": {},
   "source": [
    "**Train and evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5947fb02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/ubuntu/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 250\n",
      "/home/ubuntu/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.256287</td>\n",
       "      <td>0.898000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.4057184753417969, metrics={'train_runtime': 113.0898, 'train_samples_per_second': 35.37, 'train_steps_per_second': 2.211, 'total_flos': 1052444221440000.0, 'train_loss': 0.4057184753417969, 'epoch': 1.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb64efc2",
   "metadata": {},
   "source": [
    "### Use the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d64f5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\"This movie was pretty good.\", \"Not my cup of tea\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a07197a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tokenizer(test_data, return_tensors=\"pt\", padding=True)[\"input_ids\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c6cb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e06958d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "outputs.logits.argmax(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
