{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb3be86",
   "metadata": {},
   "source": [
    "A.S. Lundervold, v.011122"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2b9ea",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05754006",
   "metadata": {},
   "source": [
    "This is a quick example of some techniques and ideas from natural language processing (NLP) and some approaches to NLP based on deep learning. The goal is to introduce some of the things going on in this field and for you to better understand some recent ideas and developments in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46c391",
   "metadata": {},
   "source": [
    "> NLP is an exciting area these days. Breakthroughs in deep learning for language processing recently initiated a revolution in NLP, and we're still in it. The best place to start exploring this is perhaps the HuggingFace community and library (at least if you want to get started right away playing around with using state-of-the-art NLP models): https://huggingface.co/. <br> <a href=\"https://huggingface.co/\"><img width=20% src=\"https://luxcapital-website-media.s3.amazonaws.com/wp-content/uploads/2019/12/23115642/Logo-600x554.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3bd56",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39baafe6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21518cb9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17260248",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll use the excellent HuggingFace Transformers library, which covers all our natural language processing needs:\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/b253a30b83a0724f3f74f3f58236fb49ced8d7b27cb15835c9978b54e444ab08/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e67666163652f646f63756d656e746174696f6e2d696d616765732f7265736f6c76652f6d61696e2f7472616e73666f726d6572735f6c6f676f5f6e616d652e706e67\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad307da5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will not cover the library in any detail. If you're interested, take a look at the [HuggingFace course](https://huggingface.co/course/chapter1/1) and its excellent documentation over at https://huggingface.co/transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c0448",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fcf954",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll use the [IMDB dataset](https://huggingface.co/datasets/imdb) containing 50.000 movie reviews from IMDB, each labeled as either negative (0) or positive (1). It is split into 25.000 reviews for training and 25.000 reviews for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f23cb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The dataset is available via HuggingFace `datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8a3d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4a28e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aae66e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c3719",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Make a sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99509d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As the training process takes a long time, we create a small sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e3259",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee11e0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if sample:\n",
    "    dataset = dataset['train']\n",
    "    dataset = dataset.train_test_split(train_size=0.2, shuffle=True, seed=42)['train']\n",
    "    dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05875eac",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e357cc5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4942af",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The training data is stored under `train`, the test data under `test`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c430f0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are two training instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a84a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset['train'][10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86615d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can print them a in a more readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137e95c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee9562",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pprint(dataset['train'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a25a69",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **How do we represent the text for consumption by a machine learning model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d1df1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **How can a computer read??**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4793ff9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/7d5ed540c87d660cae46ca0d2055d760f786bea36513bb1a0b0784d47cef45b1/687474703a2f2f322e62702e626c6f6773706f742e636f6d2f5f2d2d75564865746b5549512f54446165356a476e6138492f4141414141414141414b302f734253704c7564576d63772f73313630302f72656164696e672e676966\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3de3a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare the data: tokenization and numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24507e5f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a computer, everything is numbers. We have to convert the text to a series of numbers and then feed those to the computer.\n",
    "\n",
    "This can be done in two widely used steps in natural language processing: **tokenization** and **numericalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106b8e4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53d947",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In tokenization, the text is split into single words called tokens. A simple way to achieve this is to separate according to spaces in the text. But then we, among other things, lose punctuation and the fact that some words are contractions of multiple words (for example \"isn't\" and \"don't\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea543c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/6c79dd15098f840a49149649832fa0efd7252d71d03257b5fc96379f7456d889/68747470733a2f2f73706163792e696f2f746f6b656e697a6174696f6e2d35376536313862643739643933336334636364333038623537333930363264362e737667\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4844a10",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Multiple tokenization strategies can tackle these and other issues, for example, **rule-based splitting of sentences** (used by ULMFiT and Transformer XL and others), **Byte-Pair encoding** (used by GPT-2 and others), **WordPiece** (used by BERT and others), and **SentencePiece** (used by XLM and others)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df9bdf9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Rule-based splitting of sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7ef09",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The NLP library `spaCy` can help us with this kind of tokenization. We install spaCy and download a set of rules for tokenizing English text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787d547",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "%pip install spacy\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b511a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a608d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be319f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "example_sentence = \"Here's a sentence to be tokenized by a tokenizer, and it includes the non-existent word graffalacticus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd65f6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(example_sentence)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d40cdf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee4f86",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5d027",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f7bee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785dd7d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Byte-Pair encoding: an example of training an encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9231a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d3feb2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96915623",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa4e7f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494501c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(dataset['train']['text'],trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5bbd0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "example_sentence_bpe = tokenizer.encode(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd7e6f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "example_sentence_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447c2b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "example_sentence_bpe.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90155e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "example_sentence_bpe.ids[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a68b6f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e7464",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We convert tokens to numbers by making a list of all the tokens that have been used and assign them to numbers. This has already been taken care of for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f107a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "example_sentence_bpe.ids[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2029eb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Embeddings and using pre-trained text encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027d8e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These concepts were introduced in the lecture using the TensorFlow Embedding Projector: http://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b51e19",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a href=\"http://projector.tensorflow.org/\"><img src=\"assets/TensorFlowProjector.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee206c0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"assets/TensorFlowProjector.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eba4d8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Some key concepts that were mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3697880",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the lecture, I told a short story about the following key concepts, widely used in modern deep learning:\n",
    "\n",
    "* Embeddings and representations\n",
    "* Word2Vec\n",
    "* Language Models\n",
    "* Training language models\n",
    "* Reusing such text representations (and similar representations of other kinds of data, including images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5d1b9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fine-tuning pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511a44a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The advent of the **Transformers models** has revolutionized the field of natural language processing. Therefore, when faced with any NLP task for which deep learning is applicable, everyone tends to turn to Transformers models. Furthermore, one typically uses _pre-trained models_. In other words, models that have already been trained on large-scale NLP tasks and thus contain representations that typically provide useful starting points for new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559300e5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Text representation for pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4b8e9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When using pre-trained models, we must pre-process the text exactly as expected by the model. In other words, that we use the expected tokenization, numericalization, padding, and truncation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858fb4c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5d212",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee4f1e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212463ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac1be6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab40400",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Fine-tune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3ee57",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll fine-tune a BERT model on our IMDB dataset. (Note that this is where it's best to use a sample of the dataset. Otherwise the training process will take a long time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c882c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c8e13",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Define the model and its preprocessing steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae28f1c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae41537",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae5148",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Set up our evaluation metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a58029",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e7f1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7eab2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Configure the training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fe61a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95c433",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#?TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc6d04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\".\", num_train_epochs=1, evaluation_strategy=\"epoch\", report_to='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a7da1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7733603",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Train and evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947fb02",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d674f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Use the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040dd6d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data = [\"This movie was pretty good.\", \"Not my cup of tea\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24186e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data = tokenizer(test_data, return_tensors=\"pt\", padding=True)[\"input_ids\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d43a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "outputs = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b1135",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "outputs.logits.argmax(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
